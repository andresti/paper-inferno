{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import neyman.models as nm\n",
    "import neyman.inferences\n",
    "\n",
    "import importlib\n",
    "\n",
    "importlib.reload(neyman.inferences)\n",
    "\n",
    "ni = neyman.inferences\n",
    "\n",
    "# add code to path\n",
    "import sys\n",
    "sys.path.append(\"../code/\")\n",
    "import inference_estimator as ie\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "ds = tf.contrib.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotation of background component parameter\n",
    "theta = tf.placeholder_with_default(0.,shape=(),name=\"theta\")\n",
    "# mean distance of background mean to (0,0)\n",
    "r_dist = tf.placeholder_with_default(2.,shape=(),name=\"r_dist\")\n",
    "\n",
    "# background distribution (no rotation and centered at (2,0))\n",
    "p0_loc = [r_dist, 0.]\n",
    "p0_cov =  [[1.,0.],[0.,9.]]\n",
    "p0 = ds.MultivariateNormalFullCovariance(loc=p0_loc,\n",
    "            covariance_matrix=p0_cov, name=\"p0\")\n",
    "\n",
    "# counter clock-wise rotation matrix\n",
    "scale = tf.linalg.LinearOperatorFullMatrix(\n",
    "            [[tf.cos(theta),-tf.sin(theta)],\n",
    "             [tf.sin(theta),tf.cos(theta)]]\n",
    "        )\n",
    "\n",
    "# background transformation of rotation (bijector)\n",
    "affine = ds.bijectors.AffineLinearOperator(scale=scale)\n",
    "\n",
    "# resulting distribution\n",
    "p0_transform = ds.TransformedDistribution(distribution=p0,\n",
    "                                         bijector=affine,\n",
    "                                         name=\"p0_transform\")\n",
    "\n",
    "# signal is a a (0,0) centered unit variance 2D Gaussian\n",
    "p1_loc =  [0., 0.]\n",
    "p1_diag = [1., 1.]\n",
    "p1 = ds.MultivariateNormalDiag(loc=p1_loc,\n",
    "                                scale_diag=p1_diag,\n",
    "                                name=\"p1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    p0_sample = sess.run(p0_transform.sample(2000), feed_dict={theta : 0.})\n",
    "    p1_sample = sess.run(p1.sample(2000))\n",
    "\n",
    "\n",
    "ax.plot(p0_sample[:,0],p0_sample[:,1],\".\")\n",
    "ax.plot(p1_sample[:,0],p1_sample[:,1],\".\")    \n",
    "\n",
    "ax.set_ylim([-10,10])\n",
    "ax.set_xlim([-10,10])\n",
    "\n",
    "fig;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 125000\n",
    "X_sample_tensors = {}\n",
    "y_values = {}\n",
    "p0_sample = \"p0_sample\"\n",
    "X_sample_tensors[p0_sample] = p0.sample(n_samples // 2, seed=7, name=\"p0_sample\") \n",
    "y_values[p0_sample] = 0.\n",
    "p1_sample = \"p1_sample\"\n",
    "X_sample_tensors[p1_sample] = p1.sample(n_samples // 2, seed=17, name=\"p1_sample\") \n",
    "y_values[p1_sample] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for name, sample_tensor in X_sample_tensors.items():\n",
    "        samples[name] = {}\n",
    "        samples[name][\"X\"] = sess.run(sample_tensor)\n",
    "        samples[name][\"y\"] =  y_values[name]*np.ones(sample_tensor.shape[0],\n",
    "                                                     dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = {}\n",
    "valid_samples = {}\n",
    "\n",
    "for name,sample in samples.items():\n",
    "    keys = sample.keys()\n",
    "    split_sample = train_test_split(*list(sample.values()),\n",
    "                                    test_size = 0.4,\n",
    "                                    random_state=17)\n",
    "    train_samples[name] = { k : split_sample[i] for k,i in zip(keys,[0,2])}\n",
    "    valid_samples[name] = { k : split_sample[i] for k,i in zip(keys,[1,3])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn_train():\n",
    "    components = {}\n",
    "    for key, value in train_samples.items():\n",
    "        components[key] = tf.data.Dataset.from_tensor_slices(value[\"X\"])\\\n",
    "                                          .shuffle(buffer_size=10000)\\\n",
    "                                          .batch(16)\n",
    "                \n",
    "    dataset = tf.data.Dataset.zip({\"components\" : components})\n",
    "    \n",
    "    dataset_it = dataset.make_one_shot_iterator()\n",
    "    next_batch = dataset_it.get_next()\n",
    "    return next_batch, None\n",
    "\n",
    "def input_fn_valid():\n",
    "    components = {}\n",
    "    for key, value in valid_samples.items():\n",
    "        components[key] = tf.data.Dataset.from_tensor_slices(value[\"X\"])\\\n",
    "                                          .shuffle(buffer_size=10000)\\\n",
    "                                          .batch(16)\n",
    "                \n",
    "    dataset = tf.data.Dataset.zip({\"components\" : components})\n",
    "    \n",
    "    dataset_it = dataset.make_one_shot_iterator()\n",
    "    next_batch = dataset_it.get_next()\n",
    "    return next_batch, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ie)\n",
    "\n",
    "def c_norm_dist_fn():\n",
    "    norm_dict = { \"p0_sample\" : 200.,       \n",
    "                  \"p1_sample\" : 200.}  \n",
    "    nuis_pars = []                   \n",
    "    return norm_dict, nuis_pars      \n",
    "\n",
    "clf = ie.InferenceEstimator(c_norm_dist_fn,\n",
    "                            n_bins =2,\n",
    "                            c_interest=\"p1_sample\",\n",
    "                            use_cross_entropy=True,\n",
    "                            model_dir=\"./dnn_clf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.train(input_fn=input_fn_train)\n",
    "clf.evaluate(input_fn=input_fn_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = tf.placeholder(dtype=tf.float32, shape=(None,2), name=\"X_ph\")\n",
    "X_prime = affine.forward(X)\n",
    "input_ph = {\"X\" : X_prime }\n",
    "\n",
    "out = clf.model_fn(input_ph, None, tf.estimator.ModeKeys.PREDICT, config=None)\n",
    "probs = out.predictions[\"probabilities\"]\n",
    "tf.train.init_from_checkpoint(clf.latest_checkpoint(), {'/' : '/'})\n",
    "init_op=  tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "theta_scan = np.linspace(-2.,2.,21, endpoint=True, dtype=np.float32)\n",
    "\n",
    "probs_arrs = {}\n",
    "X_arrs = {}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    X_arrs[p1_sample], probs_arrs[p1_sample] = sess.run((X_prime,probs[:,1]), feed_dict={X : valid_samples[p1_sample][\"X\"]})\n",
    "    probs_arrs[p0_sample] = {}\n",
    "    X_arrs[p0_sample] = {}\n",
    "    for theta_val in theta_scan:\n",
    "        X_arrs[p0_sample][theta_val], probs_arrs[p0_sample][theta_val] = sess.run((X_prime,probs[:,1]), feed_dict={X : valid_samples[p0_sample][\"X\"],\n",
    "                                                                           theta : theta_val})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "    \n",
    "bins = np.linspace(0.,1.,21, endpoint=True)\n",
    "ax.hist(probs_arrs[p0_sample][0.],bins=bins, histtype=\"step\",label=\"background\")\n",
    "ax.hist(probs_arrs[p0_sample][theta_scan[0]],  histtype=\"step\",bins=bins, label=\"background - theta -1.0\")\n",
    "ax.hist(probs_arrs[p0_sample][theta_scan[-1]],  histtype=\"step\", bins=bins, label=\"background - theta +1.0\")\n",
    "ax.hist(probs_arrs[p1_sample],bins=bins, histtype=\"step\",  label=\"signal\")\n",
    "ax.legend()\n",
    "fig;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make fine bin histograms\n",
    "hist_arrs = {}\n",
    "bins = np.linspace(0.,1.,101, endpoint=True)\n",
    "hist_arrs[\"p1_sample\"], _ = np.histogram(probs_arrs[\"p1_sample\"], bins=bins,weights=np.ones(probs_arrs[\"p1_sample\"].shape[0], dtype=np.float32))\n",
    "hist_arrs[\"p1_sample\"] /= hist_arrs[\"p1_sample\"].sum()\n",
    "hist_arrs[\"p0_sample\"] = {}\n",
    "for k,v in probs_arrs[\"p0_sample\"].items():\n",
    "    hist_arrs[\"p0_sample\"][k] , _ = np.histogram(v, bins=bins, weights=np.ones(v.shape[0], dtype=np.float32))\n",
    "    hist_arrs[\"p0_sample\"][k] /=  hist_arrs[\"p0_sample\"][k].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_arr = []\n",
    "t_arr = []\n",
    "for k,v in hist_arrs[\"p0_sample\"].items():\n",
    "    t_arr.append(k)\n",
    "    x_arr.append(np.add.reduceat(v, [0,50]))\n",
    "\n",
    "x_arr = np.array(x_arr)\n",
    "t_arr = np.array(t_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_ind = [0,50]\n",
    "c_sig = np.add.reduceat(hist_arrs[p1_sample], reduce_ind)\n",
    "c_nom = np.add.reduceat(hist_arrs[p0_sample][0.0], reduce_ind)\n",
    "c_up = np.add.reduceat(hist_arrs[p0_sample][1.0], reduce_ind)\n",
    "c_dw = np.add.reduceat(hist_arrs[p0_sample][-1.0], reduce_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_quad_lin(alpha, c_nom, c_up, c_dw):\n",
    "    \"Three-point interpolation, quadratic inside and linear outside\"\n",
    "    \n",
    "    alpha_t = tf.tile(tf.expand_dims(alpha,axis=-1),[1, tf.shape(c_nom)[0]])\n",
    "    a = 0.5*(c_up+c_dw)-c_nom\n",
    "    b = 0.5*(c_up-c_dw)\n",
    "    ones = tf.ones_like(alpha_t)\n",
    "    switch = tf.where(alpha_t < 0.,\n",
    "                      ones*tf.expand_dims(c_dw-c_nom, axis=0),\n",
    "                      ones*tf.expand_dims(c_up-c_nom, axis=0))\n",
    "    abs_var = tf.where(tf.abs(alpha_t) > 1., \n",
    "                      (2*b+tf.sign(alpha_t)*a)*(alpha_t-tf.sign(alpha_t))+switch,\n",
    "                      a*tf.pow(alpha_t,2)+b*alpha_t)\n",
    "    return c_nom+abs_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    x_arr_int = int_quad_lin(theta_scan,c_nom, c_up, c_dw).eval()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(12,6))\n",
    "\n",
    "axs[0].plot(t_arr, x_arr[:,0], label=\"true\")\n",
    "axs[0].plot(t_arr, x_arr_int[:,0], label=\"interp\")\n",
    "axs[0].legend()\n",
    "axs[1].plot(t_arr, x_arr[:,1], label=\"true\")\n",
    "axs[1].plot(t_arr, x_arr_int[:,1], label=\"interp\")\n",
    "axs[1].legend()\n",
    "\n",
    "fig;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson(x, rate):\n",
    "    \"float64  poisson pdf (avoid numerical inacurracies)\"\n",
    "    x_d = tf.cast(x, tf.float64)\n",
    "    rate_d = tf.cast(rate, tf.float64)\n",
    "    log_rate_d = tf.log(rate_d)\n",
    "    p_d = x_d*log_rate_d - tf.lgamma(tf.convert_to_tensor(1.,dtype=tf.float64)+x_d)-rate_d\n",
    "    return tf.cast( p_d, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected shapes for background (nom/up/down variation for interpolation)\n",
    "c_bkg_nom_ph = tf.placeholder(dtype=tf.float32, shape=(None,), name=\"c_bkg_nom_ph\") \n",
    "c_bkg_dw_ph = tf.placeholder(dtype=tf.float32, shape=(None,), name=\"c_bkg_dw_ph\") \n",
    "c_bkg_up_ph = tf.placeholder(dtype=tf.float32, shape=(None,), name=\"c_bkg_up_ph\") \n",
    "# expected shapes for signal (no parameters)\n",
    "c_sig_ph = tf.placeholder(dtype=tf.float32, shape=(None,), name=\"c_sig_ph\")\n",
    "\n",
    "# expected number of signal and background events\n",
    "n_bkg_ph = tf.placeholder_with_default(10000., shape=(), name=\"n_bkg_ph\")\n",
    "n_sig_ph = tf.placeholder_with_default(100., shape=(), name=\"n_sig_ph\")\n",
    "\n",
    "# model parameters (input specified by placeholders)\n",
    "mu_ph = tf.placeholder(dtype=tf.float32, shape=(None,), name=\"mu_ph\")\n",
    "theta_ph = tf.placeholder(dtype=tf.float32, shape=(None,), name=\"theta_ph\")\n",
    "\n",
    "# auxiliary measurement parameters\n",
    "theta_scale_ph = tf.placeholder_with_default(0.2, shape=(), name=\"theta_scale_ph\")\n",
    "\n",
    "# distribution of nuissance parameters\n",
    "theta_rv = nm.Normal(loc=tf.zeros_like(theta_ph),\n",
    "                     scale=tf.ones_like(theta_ph)*theta_scale_ph,\n",
    "                     value=theta_ph,\n",
    "                     name=\"theta_dist\")\n",
    "\n",
    "# background shape as a function of theta\n",
    "c_bkg = int_quad_lin(theta_rv, c_bkg_nom_ph,\n",
    "                     c_bkg_up_ph, c_bkg_dw_ph)\n",
    "\n",
    "# expected events ([batch, bin])\n",
    "mu = tf.expand_dims(mu_ph,-1, name=\"mu_expanded\")\n",
    "expected = mu*n_sig_ph*c_sig_ph+n_bkg_ph*c_bkg\n",
    "\n",
    "# placeholder for data/asimov\n",
    "observed_ph = tf.placeholder(dtype=tf.float32, shape=(None,), name=\"observed_ph\")\n",
    "\n",
    "# likelihood\n",
    "poisson_pdf = poisson(observed_ph, expected)\n",
    "nll = -tf.reduce_sum(poisson_pdf ,-1)\n",
    "theta_ext = -theta_rv.log_prob(theta_rv)\n",
    "nll_ext = nll+theta_ext\n",
    "\n",
    "# hessians and likelihoods\n",
    "h_nll, g_nll =  ni.batch_hessian(nll, pars=[mu_ph, theta_rv])\n",
    "h_nll_ext, g_nll_ext = ni.batch_hessian(nll_ext, pars=[mu_ph, theta_rv])\n",
    "\n",
    "# covariance without constraints (only POI)\n",
    "c_nll_poi = tf.matrix_inverse(h_nll[:,:1,:1], name=\"c_nll_poi\")\n",
    "c_nll = tf.matrix_inverse(h_nll, name=\"c_nll\")\n",
    "c_nll_ext = tf.matrix_inverse(h_nll_ext, name=\"c_nll_ext\")\n",
    "# profile grads and hess (only nuissance par)\n",
    "g_nll_prof = g_nll_ext[:,1:]\n",
    "h_nll_prof = h_nll_ext[:,1:,1:]\n",
    "c_nll_prof = tf.matrix_inverse(h_nll_prof, name=\"c_nll_prof\")\n",
    "\n",
    "# newton step\n",
    "newton_step =  tf.matmul(c_nll_prof, g_nll_prof[:,:, tf.newaxis])[:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_phs = {c_bkg_nom_ph : c_nom,\n",
    "             c_bkg_dw_ph : c_dw,\n",
    "             c_bkg_up_ph : c_up,\n",
    "             c_sig_ph : c_sig}\n",
    "\n",
    "norm_phs = {n_bkg_ph : 10000.,\n",
    "            n_sig_ph : 100.}\n",
    "\n",
    "par_phs = {mu_ph : [1.],\n",
    "           theta_ph: [0.]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "par_phs = {mu_ph : [1.],\n",
    "           theta_ph: [0.]}\n",
    "\n",
    "feed_dict = {**shape_phs, **norm_phs, **par_phs}\n",
    "\n",
    "minima = {}\n",
    "with tf.Session() as sess:\n",
    "    asimov_data = sess.run(expected, feed_dict=feed_dict)\n",
    "    print(\"asimov data: \", asimov_data[0])\n",
    "    asimov_phs = {observed_ph : asimov_data[0]}\n",
    "    feed_dict = {**feed_dict, **asimov_phs}\n",
    "    minima  = sess.run({ t : t for t in [c_nll_ext, c_nll_poi, g_nll_ext ] }, feed_dict=feed_dict)\n",
    "\n",
    "print(\"error mu no nuis:\", np.sqrt(np.diag(minima[c_nll_poi][0])))\n",
    "print(\"error mu and nuis:\", np.sqrt(np.diag(minima[c_nll_ext][0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "theta_scan = np.linspace(-0.2,0.2, 101, endpoint=True, dtype=np.float32)\n",
    "par_phs = {mu_ph : [1.0],\n",
    "           theta_ph: theta_scan }\n",
    "\n",
    "feed_dict = {**shape_phs, **norm_phs, **par_phs, **asimov_phs}\n",
    "\n",
    "arrs = {}\n",
    "with tf.Session() as sess:\n",
    "    feed_dict = {**feed_dict, **asimov_phs}\n",
    "    arrs[theta_ext]= sess.run(theta_ext, feed_dict=feed_dict)\n",
    "    arrs[nll_ext] = sess.run(nll_ext, feed_dict=feed_dict)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "ax.plot(theta_scan, arrs[theta_ext]-arrs[theta_ext].min())\n",
    "ax.plot(theta_scan, arrs[nll_ext]-arrs[nll_ext].min())\n",
    "\n",
    "fig;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_scan = np.linspace(0.2,1.5, 101, endpoint=True, dtype=np.float32)\n",
    "par_phs = {mu_ph : mu_scan,\n",
    "           theta_ph: np.zeros_like(mu_scan)}\n",
    "\n",
    "feed_dict = {**shape_phs, **norm_phs, **par_phs, **asimov_phs}\n",
    "\n",
    "no_nuis = {}\n",
    "profiled = {}\n",
    "with tf.Session() as sess:\n",
    "    no_nuis[nll_ext] = sess.run(nll_ext, feed_dict=feed_dict)\n",
    "    feed_dict[theta_ph] = feed_dict[theta_ph]+0.1\n",
    "    for i in range(10):\n",
    "        newton_step_arr = sess.run(newton_step , feed_dict=feed_dict)\n",
    "        feed_dict[theta_ph] =  feed_dict[theta_ph]-newton_step_arr\n",
    "    profiled[nll_ext] = sess.run(nll_ext, feed_dict=feed_dict)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "ax.plot(mu_scan, profiled[nll_ext]-profiled[nll_ext].min())\n",
    "ax.plot(mu_scan, no_nuis[nll_ext]-no_nuis[nll_ext].min())\n",
    "\n",
    "fig;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mu_scan = np.linspace(0.2,1.5, 51, endpoint=True, dtype=np.float32)\n",
    "theta_scan = np.linspace(-0.2,0.2, 101, endpoint=True, dtype=np.float32)\n",
    "par_phs = {mu_ph : [1],\n",
    "           theta_ph: theta_scan}\n",
    "\n",
    "feed_dict = {**shape_phs, **norm_phs, **par_phs, **asimov_phs}\n",
    "\n",
    "nll_surface = np.empty([mu_scan.shape[0],theta_scan.shape[0]], dtype=np.float32)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i, mu_val in enumerate(mu_scan):\n",
    "        feed_dict[mu_ph] = [mu_val]\n",
    "        nll_surface[i] = sess.run(nll_ext, feed_dict=feed_dict)\n",
    "\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "#im = ax.imshow(nll_surface)\n",
    "pcm = ax.pcolor(theta_scan,mu_scan,nll_surface-nll_surface.min())\n",
    "fig.colorbar(pcm, ax=ax)\n",
    "\n",
    "fig;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
