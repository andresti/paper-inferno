{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic setup for Colab\n",
    "import os\n",
    "LOG_DIR = 'log_dir/.'\n",
    "\n",
    "if 'DATALAB_ENV' in os.environ:\n",
    "  import getpass\n",
    "  gitlab_token = getpass.getpass()\n",
    "  !git clone https://oauth2:{gitlab_token}@gitlab.cern.ch/pdecastr/paper-learning_inference.git\n",
    "  !git clone https://oauth2:{gitlab_token}@gitlab.cern.ch/pdecastr/neyman.git\n",
    "  !pip install git+https://github.com/wookayin/tensorflow-plot.git@master\n",
    "  !pip install neyman/.\n",
    "  repo_path = \"paper-learning_inference\"\n",
    "  !npm install -g localtunnel\n",
    "  get_ipython().system_raw(\n",
    "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
    "    .format(LOG_DIR)\n",
    "  )\n",
    "  get_ipython().system_raw('lt --port 6006 >> url.txt 2>&1 &')\n",
    "  !cat url.txt\n",
    "else:\n",
    "  repo_path = \"..\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "# add code to path\n",
    "import sys\n",
    "sys.path.append(f\"{repo_path}/code/\")\n",
    "from scipy.interpolate import InterpolatedUnivariateSpline\n",
    "import inference_estimator as ie\n",
    "from template_likelihood import TemplateLikelihood\n",
    "import neyman.models as nm\n",
    "import tensorflow as tf\n",
    "import neyman.inferences as ni\n",
    "import neyman.models as nm\n",
    "\n",
    "\n",
    "k = tf.keras\n",
    "\n",
    "font = {'size'   : 14}\n",
    "\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_path = \"../data/2d_gaussian/valid/*\"\n",
    "test_path = \"../data/2d_gaussian/test/*\"\n",
    "\n",
    "valid_samples = {}\n",
    "for f in glob(valid_path):\n",
    "  name = os.path.splitext(os.path.basename(f))[0]\n",
    "  valid_samples[name] = dict(np.load(f))\n",
    "\n",
    "test_samples = {}\n",
    "for f in glob(test_path):\n",
    "  name = os.path.splitext(os.path.basename(f))[0]\n",
    "  test_samples[name] = dict(np.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = TemplateLikelihood()\n",
    "\n",
    "expected = tl.expected()\n",
    "nll = tl.nll()\n",
    "h_nll, g_nll = ni.batch_hessian(nll, pars=tl.pars)\n",
    "\n",
    "# covariance\n",
    "c_nll = tf.matrix_inverse(h_nll, name=\"c_nll\")\n",
    "# profile grads and hess (only nuissance par)\n",
    "g_nll_prof = g_nll[:,1:]\n",
    "h_nll_prof = h_nll[:,1:,1:]\n",
    "c_nll_prof = tf.matrix_inverse(h_nll_prof, name=\"c_nll_prof\")\n",
    "\n",
    "# newton step\n",
    "newton_step =  tf.matmul(c_nll_prof, g_nll_prof[:,:, tf.newaxis])[:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_asimov(feed_dict):\n",
    "    feed_dict = feed_dict.copy()\n",
    "    with tf.Session() as sess:\n",
    "      asimov_data = sess.run(expected, feed_dict=feed_dict)\n",
    "    return asimov_data[0]\n",
    "\n",
    "def get_hessian(feed_dict):\n",
    "  feed_dict = feed_dict.copy()\n",
    "  with tf.Session() as sess:    \n",
    "    hess = sess.run(h_nll, feed_dict=feed_dict)\n",
    "  return hess\n",
    "\n",
    "def get_unc_approx(feed_dict):\n",
    "  feed_dict = feed_dict.copy()\n",
    "  with tf.Session() as sess:    \n",
    "    cov  = sess.run(c_nll, feed_dict=feed_dict)\n",
    "  return np.sqrt(cov[0])[0]\n",
    "\n",
    "def get_unc_profile(feed_dict):\n",
    "  feed_dict = feed_dict.copy()\n",
    "  with tf.Session() as sess:\n",
    "    no_nuis = sess.run(nll, feed_dict=feed_dict)\n",
    "    r_dist_ph = tl.phs[\"r_dist\"]\n",
    "    feed_dict[r_dist_ph] = feed_dict[r_dist_ph]\n",
    "    for i in range(10):\n",
    "        newton_step_arr = sess.run(newton_step , feed_dict=feed_dict)\n",
    "        feed_dict[r_dist_ph] =  feed_dict[r_dist_ph]-newton_step_arr\n",
    "    profiled = sess.run(nll, feed_dict=feed_dict)\n",
    "  return no_nuis, profiled\n",
    "\n",
    "\n",
    "def get_nll_surface(feed_dict, mu_scan):\n",
    "  feed_dict = feed_dict.copy()\n",
    "  nll_surface = np.empty([mu_scan.shape[0],r_dist_scan.shape[0]], dtype=np.float32)\n",
    "  mu_ph = tl.phs[\"mu\"]\n",
    "  with tf.Session() as sess:\n",
    "    for i, mu_val in enumerate(mu_scan):\n",
    "      feed_dict[mu_ph] = [mu_val]\n",
    "      nll_surface[i] = sess.run(nll, feed_dict=feed_dict)\n",
    "  return nll_surface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_uncs = {}\n",
    "profile_uncs = {}\n",
    "hists_all = {}\n",
    "surfaces = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 2\n",
    "shift = np.array([[0.2,0.]])\n",
    "bins = np.linspace(0., 1., 11, endpoint=True)\n",
    "\n",
    "re_ce = \"fix/cross_entropy_b_2_bs_128_lr_0.01_seed_*0.h5\"\n",
    "\n",
    "for model_path in glob(re_ce):\n",
    "  \n",
    "  norm_phs = {tl.phs[\"n_bkg\"] : 960.,\n",
    "              tl.phs[\"n_sig\"] : 40.}\n",
    "\n",
    "  par_phs = {tl.phs[\"mu\"] : [1.],\n",
    "             tl.nuis_rv: [2.]}\n",
    "  \n",
    "  model = k.models.load_model(model_path)\n",
    "  model_name = os.path.splitext(model_path)[0]\n",
    "  \n",
    "  preds_test = {}\n",
    "  preds_test[\"bkg_nom\"] = model.predict_proba(test_samples[\"p0_sample\"][\"X\"])[:,1]\n",
    "  preds_test[\"sig\"] = model.predict_proba(test_samples[\"p1_sample\"][\"X\"])[:,1]\n",
    "  preds_test[\"bkg_up\"] = model.predict_proba(test_samples[\"p0_sample\"][\"X\"]+shift)[:,1]\n",
    "  preds_test[\"bkg_dw\"] = model.predict_proba(test_samples[\"p0_sample\"][\"X\"]-shift)[:,1]\n",
    "  \n",
    "  hists_test = { k : np.histogram(v, bins=bins)[0] for k,v in preds_test.items()}\n",
    "  hists_test = { k : v/v.sum() for k,v in hists_test.items()}\n",
    "  hists_all[model_name] = hists_test\n",
    "  \n",
    "  not_zero = np.any([h != 0.0 for h in hists_test.values()],axis=0)\n",
    "  shape_phs = {tl.phs[\"c_bkg_nom\"] : hists_test[\"bkg_nom\"][not_zero],\n",
    "               tl.phs[\"c_bkg_dw\"]  : hists_test[\"bkg_dw\"][not_zero],\n",
    "               tl.phs[\"c_bkg_up\"]  : hists_test[\"bkg_up\"][not_zero],\n",
    "               tl.phs[\"c_sig\"] : hists_test[\"sig\"][not_zero]}\n",
    "  \n",
    "  feed_dict = {**shape_phs, **norm_phs, **par_phs}\n",
    "  asimov_data = get_asimov(feed_dict)\n",
    "  asimov_phs = {tl.phs[\"observed\"] : asimov_data}\n",
    "  feed_dict = {**shape_phs, **norm_phs, **par_phs, **asimov_phs}\n",
    "\n",
    "  approx_uncs[model_name] = get_unc_approx(feed_dict)\n",
    "  mu_scan = np.linspace(0.05,1.95, 101, endpoint=True, dtype=np.float32)\n",
    "  par_phs = {tl.phs[\"mu\"] : mu_scan,\n",
    "             tl.phs[\"r_dist\"] : np.ones_like(mu_scan)*2.}\n",
    "  feed_dict = {**shape_phs, **norm_phs, **par_phs, **asimov_phs}\n",
    "  profile_uncs[model_name] = get_unc_profile(feed_dict)\n",
    "  r_dist_scan = np.linspace(1.2,2.8, 101, endpoint=True, dtype=np.float32)\n",
    "  par_phs = {tl.phs[\"mu\"] : [1],\n",
    "             tl.phs[\"r_dist\"] : r_dist_scan}\n",
    "  feed_dict = {**shape_phs, **norm_phs, **par_phs, **asimov_phs}\n",
    "  surfaces[model_name] = get_nll_surface(feed_dict, mu_scan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 10\n",
    "shift = np.array([[0.2,0.]])\n",
    "\n",
    "re_inferno = \"fix/inferno_b_10_bs_512_lr_0.0001_seed_*.h5\"\n",
    "\n",
    "for model_path in glob(re_inferno):\n",
    "  \n",
    "  norm_phs = {tl.phs[\"n_bkg\"] : 960.,\n",
    "              tl.phs[\"n_sig\"] : 40.}\n",
    "\n",
    "  par_phs = {tl.phs[\"mu\"] : [1.],\n",
    "             tl.nuis_rv: [2.]}\n",
    "\n",
    "  model = k.models.load_model(model_path)\n",
    "  model_name = os.path.splitext(model_path)[0]\n",
    "  \n",
    "  preds_test = {}\n",
    "  preds_test[\"bkg_nom\"] = model.predict_proba(test_samples[\"p0_sample\"][\"X\"])\n",
    "  preds_test[\"sig\"] = model.predict_proba(test_samples[\"p1_sample\"][\"X\"])\n",
    "  preds_test[\"bkg_up\"] = model.predict_proba(test_samples[\"p0_sample\"][\"X\"]+shift)\n",
    "  preds_test[\"bkg_dw\"] = model.predict_proba(test_samples[\"p0_sample\"][\"X\"]-shift)\n",
    "  \n",
    "  hists_test_raw = { k : np.bincount(np.argmax(v, axis=-1),minlength=n_bins) for k,v in preds_test.items()}\n",
    "  hists_test = { k : v/v.sum() for k,v in hists_test_raw.items()}\n",
    "  \n",
    "  hists_all[model_name] = hists_test\n",
    "  \n",
    "  not_zero = np.any([h != 0.0 for h in hists_test.values()],axis=0)\n",
    "\n",
    "\n",
    "  shape_phs = {tl.phs[\"c_bkg_nom\"] : hists_test[\"bkg_nom\"][not_zero],\n",
    "               tl.phs[\"c_bkg_dw\"]  : hists_test[\"bkg_dw\"][not_zero],\n",
    "               tl.phs[\"c_bkg_up\"]  : hists_test[\"bkg_up\"][not_zero],\n",
    "               tl.phs[\"c_sig\"] : hists_test[\"sig\"][not_zero]}\n",
    "  \n",
    "  feed_dict = {**shape_phs, **norm_phs, **par_phs}\n",
    "  asimov_data = get_asimov(feed_dict)\n",
    "  asimov_phs = {tl.phs[\"observed\"] : asimov_data}\n",
    "  feed_dict = {**shape_phs, **norm_phs, **par_phs, **asimov_phs}\n",
    "\n",
    "  approx_uncs[model_name] = get_unc_approx(feed_dict)\n",
    "  mu_scan = np.linspace(0.05,1.95, 101, endpoint=True, dtype=np.float32)\n",
    "  par_phs = {tl.phs[\"mu\"] : mu_scan,\n",
    "             tl.phs[\"r_dist\"] : np.ones_like(mu_scan)*2.}\n",
    "  feed_dict = {**shape_phs, **norm_phs, **par_phs, **asimov_phs}\n",
    "  profile_uncs[model_name] = get_unc_profile(feed_dict)\n",
    "  r_dist_scan = np.linspace(1.2,2.8, 101, endpoint=True, dtype=np.float32)\n",
    "  par_phs = {tl.phs[\"mu\"] : [1],\n",
    "             tl.phs[\"r_dist\"] : r_dist_scan}\n",
    "  feed_dict = {**shape_phs, **norm_phs, **par_phs, **asimov_phs}\n",
    "  surfaces[model_name] = get_nll_surface(feed_dict, mu_scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "levels = [0.,0.5,1.0,2.0,4]\n",
    "\n",
    "key = 'progress/cross_entropy_b_2_bs_128_lr_0.01_seed_167_19'\n",
    "\n",
    "ax.contourf(r_dist_scan, mu_scan,surfaces[key]-surfaces[key].min(),\n",
    "            levels=levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "unc_roots = {}\n",
    "for name, prof in profile_uncs.items():\n",
    "  is_not_nan = (np.isnan(prof[1]) == False)\n",
    "  profile = prof[1][is_not_nan]-(prof[1][is_not_nan]).min()\n",
    "  \n",
    "  if \"fix/cross_entropy\" in name:\n",
    "    l_ce = ax.plot(mu_scan[is_not_nan], profile,\"r-\", alpha=0.2)\n",
    "    unc_roots[name] =  InterpolatedUnivariateSpline(mu_scan, profile-0.5).roots()\n",
    "  if (\"fix/inferno_b_10_bs_512_lr_0.0001_\" in name) and (\"150\" not in name):\n",
    "    l_inferno = ax.plot(mu_scan[is_not_nan], profile,\"b-\", alpha=0.2)\n",
    "    unc_roots[name] =  InterpolatedUnivariateSpline(mu_scan[is_not_nan], profile-0.5).roots()\n",
    "  elif (\"150\" not in name):\n",
    "    unc_roots[name] =  InterpolatedUnivariateSpline(mu_scan[is_not_nan], profile-0.5).roots()\n",
    "\n",
    "\n",
    "ax.set_ylim([0.,1.0])\n",
    "ax.set_xlim([0.25,1.75])\n",
    "\n",
    "\n",
    "ax.set_xlabel(r\"signal strength parameter $\\nu$\")\n",
    "ax.set_ylabel(r\"likelihood profile $\\Delta\\mathcal{L}$\")\n",
    "\n",
    "unc_width = { k : (v[1]-v[0])/2.  for k,v in unc_roots.items()}\n",
    "\n",
    "ax.legend((l_ce[0], l_inferno[0]), (\"cross-entropy\",\"inference-aware\"),frameon=False)\n",
    "fig.savefig(\"profile_likelihood.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_losses = {} \n",
    "\n",
    "re_no_ext = os.path.splitext(re_inferno)[0]\n",
    "print(re_no_ext)\n",
    "for event_path in glob(f\"log_dir_{re_no_ext}/eval/events.out.tfevents.*\"):\n",
    "  print(event_path)\n",
    "  model_name = event_path.split(\"/\")[1]\n",
    "  if (any(ss in model_name for ss in [\"77\",\"150\"])):\n",
    "    continue\n",
    "  eval_losses[model_name] = {\"steps\" : [],\n",
    "                             \"asimov_loss\" : []}\n",
    "\n",
    "  for e in tf.train.summary_iterator(event_path):\n",
    "    for v in e.summary.value:\n",
    "      if v.tag == \"asimov_loss\":\n",
    "        eval_losses[model_name][\"steps\"].append(e.step)\n",
    "        eval_losses[model_name][\"asimov_loss\"].append(v.simple_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_losses.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_losses = {} \n",
    "\n",
    "re_no_ext = os.path.splitext(re_ce)[0]\n",
    "print(re_no_ext)\n",
    "for event_path in glob(f\"log_dir_fix/cross_entropy_b_2_bs_128_lr_0.01_seed_167/eval/events.out.tfevents.*\"):\n",
    "  print(event_path)\n",
    "  model_name = event_path.split(\"/\")[1]\n",
    "  eval_losses[model_name] = {\"steps\" : [],\n",
    "                             \"asimov_loss\" : []}\n",
    "\n",
    "  for e in tf.train.summary_iterator(event_path):\n",
    "    for v in e.summary.value:\n",
    "      if v.tag == \"asimov_loss\":\n",
    "        eval_losses[model_name][\"steps\"].append(e.step)\n",
    "        eval_losses[model_name][\"asimov_loss\"].append(v.simple_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_name='cross_entropy_b_2_bs_128_lr_0.01_seed_167'\n",
    "eval_ce = eval_losses[base_name][\"asimov_loss\"][1:]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "l_soft_std = { key : np.sqrt(v[\"asimov_loss\"][-1]) for key, v in eval_losses.items()}\n",
    "pl_vals = []\n",
    "ap_vals = []\n",
    "lo_vals = []\n",
    "for i in range(20):\n",
    "  key = f\"progress/{base_name}_{i}\"\n",
    "  pl_vals.append(unc_width[key])\n",
    "  ap_vals.append(approx_uncs[key][1])\n",
    "\n",
    "lo_vals = np.sqrt(eval_ce)\n",
    "\n",
    "ax.plot(pl_vals, lo_vals, \"-\", label=\"from loss function\")\n",
    "ax.plot(pl_vals, ap_vals, \"-\", label= \"from template likelihood\")\n",
    "\n",
    "ax.set_xlabel(\"profile likelihood width\")\n",
    "ax.set_ylabel(\"hessian approximation\")\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "#fig.savefig(\"pl_vs_hessian_diagnosis_ce_dynamics.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "\n",
    "ax.plot(range(20),pl_vals, label = \"template pf\",)\n",
    "ax.plot(range(20),ap_vals, label = \"template unc hess\")\n",
    "ax.plot(range(20),lo_vals, label = \"loss unc hess\")\n",
    "\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "for eval_loss in eval_losses.values():\n",
    "  ax.plot(eval_loss[\"steps\"],eval_loss[\"asimov_loss\"])\n",
    "\n",
    "ax.set_xlabel(\"training step\")\n",
    "ax.set_ylabel(\"validation-set inference-aware loss\")\n",
    "\n",
    "fig.savefig(\"training_dynamics.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "l_soft_std = { k : np.sqrt(v[\"asimov_loss\"][-1]) for k, v in eval_losses.items()}\n",
    "pl_vals = []\n",
    "ap_vals = []\n",
    "lo_vals = []\n",
    "for k in unc_width:\n",
    "  if \"inferno\" in k:\n",
    "    model_name = k.split(\"/\")[1]\n",
    "    lo_vals.append(l_soft_std[model_name])\n",
    "    pl_vals.append(unc_width[k])\n",
    "    ap_vals.append(approx_uncs[k][0])\n",
    "    \n",
    "ax.plot(pl_vals, lo_vals, \"o\", label=\"from loss function\")\n",
    "ax.plot(pl_vals, ap_vals, \"o\", label= \"from template likelihood\")\n",
    "\n",
    "ax.set_xlabel(\"profile likelihood width\")\n",
    "ax.set_ylabel(\"hessian approximation\")\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "fig.savefig(\"pl_vs_hessian_diagnosis.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subset in [\"cross\", \"inferno\"]:\n",
    "  mean = np.mean([v for n,v in unc_width.items() if subset in n])\n",
    "  std = np.std([v for n,v in unc_width.items() if subset in n],ddof=1)\n",
    "  print(subset,mean,std)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
