{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic setup for Colab\n",
    "import os\n",
    "LOG_DIR = 'log_dir_fix/.'\n",
    "\n",
    "if 'DATALAB_ENV' in os.environ:\n",
    "  import getpass\n",
    "  gitlab_token = getpass.getpass()\n",
    "  !git clone https://oauth2:{gitlab_token}@gitlab.cern.ch/pdecastr/paper-learning_inference.git\n",
    "  !git clone https://oauth2:{gitlab_token}@gitlab.cern.ch/pdecastr/neyman.git\n",
    "  !pip install git+https://github.com/wookayin/tensorflow-plot.git@master\n",
    "  !pip install neyman/.\n",
    "  repo_path = \"paper-learning_inference\"\n",
    "  !npm install -g localtunnel\n",
    "  get_ipython().system_raw(\n",
    "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
    "    .format(LOG_DIR)\n",
    "  )\n",
    "  get_ipython().system_raw('lt --port 6006 >> url.txt 2>&1 &')\n",
    "  !cat url.txt\n",
    "else:\n",
    "  repo_path = \"..\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "\n",
    "# add code to path\n",
    "import sys\n",
    "sys.path.append(f\"{repo_path}/code/\")\n",
    "import inference_estimator as ie\n",
    "import neyman.models as nm\n",
    "import tensorflow as tf\n",
    "\n",
    "k = tf.keras\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../data/2d_gaussian/train/*\"\n",
    "valid_path = \"../data/2d_gaussian/valid/*\"\n",
    "\n",
    "train_samples = {}\n",
    "for f in glob(train_path):\n",
    "  name = os.path.splitext(os.path.basename(f))[0]\n",
    "  train_samples[name] = dict(np.load(f))\n",
    "\n",
    "valid_samples = {}\n",
    "for f in glob(valid_path):\n",
    "  name = os.path.splitext(os.path.basename(f))[0]\n",
    "  valid_samples[name] = dict(np.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_fn(samples, batch_size, shuffle=True,seed=None):\n",
    "    def input_fn():\n",
    "        components = {}\n",
    "        for key, value in samples.items():\n",
    "            tensor = tf.data.Dataset.from_tensor_slices(value[\"X\"])\n",
    "            if shuffle: tensor = tensor.shuffle(buffer_size=10000, seed=None)\n",
    "            components[key] = tensor.batch(batch_size)\n",
    "                \n",
    "        dataset = tf.data.Dataset.zip({\"components\" : components})\n",
    "    \n",
    "        dataset_it = dataset.make_one_shot_iterator()\n",
    "        next_batch = dataset_it.get_next()\n",
    "        return next_batch, None\n",
    "    \n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_norm_dist_fn():\n",
    "    norm_dict = { \"p0_sample\" : 960.,       \n",
    "                  \"p1_sample\" : 40.}  \n",
    "    nuis_pars = []                   \n",
    "    return norm_dict, nuis_pars\n",
    "\n",
    "def c_transforms_fn(rv_collections=[]):\n",
    "    shift = nm.Normal(loc=0.,scale=0.2,value=0.0, name=\"shift\",\n",
    "        collections=rv_collections)\n",
    "    shift_2d = tf.stack([shift,0.],axis=0)\n",
    "    trans_dict = {\"p0_sample\" : lambda t: t+shift_2d,\n",
    "                  \"p1_sample\" : lambda t: t}\n",
    "    nuis_pars = [shift]\n",
    "    return trans_dict,nuis_pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_dict_def = {\"prefix\" : f\"{LOG_DIR}/cross_entropy\",\n",
    "              \"n_bins\" : 2,\n",
    "              \"batch_size\" : 128,\n",
    "              \"eval_batch_size\" : 10000,\n",
    "              \"learning_rate\" : 0.01,\n",
    "              \"seed\" : None,\n",
    "              \"temperature\" : 0.01,\n",
    "              \"max_steps\" : 5000,\n",
    "              \"save_steps\" : 10,\n",
    "              \"use_cross_entropy\" : True,\n",
    "              \"n_epochs\" : 50,\n",
    "              \"export_each_epoch\" : False,\n",
    "              \"control_plots\" : False\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(c_dict):\n",
    "  \n",
    "  eval_list = []\n",
    "  \n",
    "  if \"model_dir\" not in c_dict:\n",
    "    model_dir = \"{}_b_{}_bs_{}_lr_{}_seed_{}\".format(\n",
    "      c_dict[\"prefix\"],\n",
    "      c_dict[\"n_bins\"],\n",
    "      c_dict[\"batch_size\"],\n",
    "      c_dict[\"learning_rate\"],\n",
    "      c_dict[\"seed\"],\n",
    "      c_dict[\"temperature\"])\n",
    "    c_dict[\"model_dir\"] = model_dir\n",
    "  \n",
    "  \n",
    "  run_config = tf.estimator.RunConfig(save_summary_steps=c_dict[\"save_steps\"],\n",
    "                                      tf_random_seed = c_dict[\"seed\"])\n",
    "\n",
    "  estimator = ie.InferenceEstimator(ie.small_nn,\n",
    "                                    c_norm_dist_fn,\n",
    "                                    c_transforms_fn=c_transforms_fn,\n",
    "                                    n_bins=c_dict[\"n_bins\"],\n",
    "                                    learning_rate=c_dict[\"learning_rate\"],\n",
    "                                    c_interest=\"p1_sample\",\n",
    "                                    use_cross_entropy=c_dict[\"use_cross_entropy\"],\n",
    "                                    temperature=c_dict[\"temperature\"],\n",
    "                                    model_dir=c_dict[\"model_dir\"],\n",
    "                                    control_plots=c_dict[\"control_plots\"],\n",
    "                                    config=run_config)\n",
    "  \n",
    "  train_fn = create_input_fn(train_samples, c_dict[\"batch_size\"])\n",
    "  valid_fn = create_input_fn(valid_samples, c_dict[\"eval_batch_size\"], shuffle=False)\n",
    "  estimator.train(train_fn, max_steps=1)\n",
    "  estimator.evaluate(valid_fn)\n",
    "  \n",
    "  rs = np.random.RandomState(seed=c_dict[\"seed\"])\n",
    "  \n",
    "  for epoch in range(c_dict[\"n_epochs\"]):\n",
    "    s_seed = rs.randint(100000)\n",
    "    train_fn = create_input_fn(train_samples, c_dict[\"batch_size\"],seed=s_seed)\n",
    "    valid_fn = create_input_fn(valid_samples, c_dict[\"eval_batch_size\"], shuffle=False)\n",
    "    estimator.train(train_fn)\n",
    "    eval_list.append(estimator.evaluate(valid_fn))\n",
    "    if c_dict[\"export_each_epoch\"]:\n",
    "      export_to_keras(c_dict[\"model_dir\"], f\"progress\", epoch)\n",
    "\n",
    "  timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "  with open(f'{model_dir}/{timestr}.json', 'w') as fp:\n",
    "    json.dump(c_dict, fp)\n",
    "\n",
    "  return eval_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_dict = c_dict_def.copy()\n",
    "c_dict[\"export_each_epoch\"] = True\n",
    "c_dict[\"seed\"] = 167\n",
    "c_dict[\"n_epochs\"] = 20\n",
    "c_dict[\"conntrol_plots\"] = True\n",
    "eval_log = train_and_evaluate(c_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in np.linspace(10,100,10, dtype=np.int32):\n",
    "  c_dict = c_dict_def.copy()\n",
    "  c_dict[\"seed\"] = int(seed)\n",
    "  print(c_dict)\n",
    "  train_and_evaluate(c_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "folders = glob(\"log_dir_fix/cross_entropy_b_2*\")\n",
    "n_bins = 2\n",
    "output_folder = \"fix\"\n",
    "\n",
    "for folder in folders:\n",
    "  export_to_keras(folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def export_to_keras(model_folder, output_folder, suffix = \"last\"):\n",
    "  print(model_folder)\n",
    "  model_dir = os.path.split(model_folder)[-1]\n",
    "  with open(f\"{model_folder}/checkpoint\") as f:\n",
    "    checkp =  yaml.load(f)[\"model_checkpoint_path\"]\n",
    "\n",
    "  graph = tf.Graph()\n",
    "  with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "      k.backend.set_session(sess)\n",
    "    \n",
    "      with tf.variable_scope(model_dir):\n",
    "          model = ie.small_nn(n_logits=n_bins,softmax_output=True)\n",
    "    \n",
    "      # so it can be saved\n",
    "      model.compile(\"SGD\",\"sparse_categorical_crossentropy\")\n",
    "    \n",
    "      print(f\"checkpoint: {model_folder}/{checkp}\")\n",
    "      tf.train.init_from_checkpoint(f\"{model_folder}/{checkp}\", {'/' : \"{}/\".format(model_dir)})\n",
    "      init_op =  tf.global_variables_initializer()\n",
    "    \n",
    "      sess.run(init_op)\n",
    "      \n",
    "      print(f\"output: {output_folder}/{model_dir}_{suffix}.h5\")\n",
    "      model.save(f\"{output_folder}/{model_dir}_{suffix}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
